{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "PAGE 1 \n",
    "\n",
    "Le user arrive sur la page streamlit.\n",
    "\n",
    "On propose une page de classement des films par :\n",
    "* Ordre alphabétique\n",
    "* par réalisateur\n",
    "* par acteurs / actrices\n",
    "* par genres\n",
    "* par popularité (moyenne bayesian du rating)\n",
    "* par année (et ordre alphabétique)\n",
    "* par durée\n",
    "* etc.\n",
    "\n",
    "Tout ça constitue une première page du site.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Item-Item = kNN / User-Item : SVD / Deep\n",
    "\n",
    "PAGE 2\n",
    "\n",
    "Sur une seconde page, on propose une catégorie : mes films / ma bilbiothèque, etc. whatever --> Recommandations personnelles.\n",
    "\n",
    "CAS 1 - L'utilisateur novice : Donne la liste de ses genres de films préférés. Eventuellement acteurs / actrices / (réalisateurs?). \n",
    "On crée un film fictif --> similarité cosinus) --> On sort les k films les plus proches. On les notes + mieux notés bayesian.\n",
    "\n",
    "FAIT\n",
    "\n",
    "\n",
    "CAS 2 -  L'utilisateur medium : Il donne un film (ou plusieurs) qu'il aime. En fonction de ce qu'il donne on complète la liste avec du collaborative \n",
    "filtering item-item kNN. et l'algo lui renvoie une liste des k films qu'il aimera.\n",
    " \n",
    "\n",
    " \n",
    "CAS 3 -  L'utilisateur cinéphile : Fait une notation sur un échantillon conséquent de films (de son choix ? qu'on lui propose). A partir de ses notations, on sort les meilleurs films. \n",
    "Approche deep learning simple : collaborative filtering user-item ou\n",
    "va aimer. --> collaborative filtering (Deep learning ou SVD)\n",
    "\n",
    "\n",
    "\n",
    "(Possibilité de faire de l'hybride dans toutes les cas, mais il faut à chaque fois pondérer différemment)\n",
    "\n",
    "Les k plus proches voisins sont une méthode de base pour le collaborative filtering basée sur du item-item. A la place, on peut utiliser la factorisation\n",
    "matricielle avec SVD, ou encore le deep learning.\n",
    "\n",
    "Pour le content based filtering, on se contente de la matrice de similarité cosinus.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "On récupère toutes ces informations.\n",
    "\n",
    "On cherche les k plus proches voisins des films donnés \n",
    "On cherche les k films les mieux notés qui pro\n",
    "\n",
    "\n",
    "\n",
    " '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('movies.csv')\n",
    "ratings = pd.read_csv('ratings.csv')\n",
    "links = pd.read_csv('links.csv')\n",
    "\n",
    "title_principals = pd.read_csv('data-7.tsv', sep = '\\t') # tconst\tordering\tnconst\tcategory\tjob\tcharacters 38.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faire le lien avec la base imdb\n",
    "\n",
    "movies = movies.merge(right = links, on = 'movieId', how = 'left')\n",
    "movies.imdbId = movies.imdbId.apply(lambda x : 'tt' + (7 - len(str(x))) * str(0) + str(x))\n",
    "movies = movies.drop(columns='tmdbId')\n",
    "movies = movies.rename(columns = {'imdbId' : 'tconst'})\n",
    "# Le lien est fait avec la base imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothèses\n",
    "nombre_de_films = 1500\n",
    "nb_users = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée un DataFrame movie_stats qui contient pour chaque ligne le film et la moyenne bayesian des notations\n",
    "\n",
    "movie_stats = ratings.groupby('movieId')[['rating']].agg(['count', 'mean'])\n",
    "movie_stats.columns = movie_stats.columns.droplevel()\n",
    "\n",
    "C = movie_stats['count'].mean()\n",
    "m = movie_stats['mean'].mean()\n",
    "\n",
    "def bayesian_avg(ratings):\n",
    "    bayesian_avg = (C*m+ratings.sum())/(C+ratings.count())\n",
    "    return bayesian_avg\n",
    "\n",
    "bayesian_avg_ratings = ratings.groupby(['movieId'])['rating'].agg(bayesian_avg).reset_index()\n",
    "bayesian_avg_ratings.columns = ['movieId', 'bayesian_avg']\n",
    "movie_stats = movie_stats.merge(bayesian_avg_ratings, on='movieId')\n",
    "\n",
    "movie_stats = movie_stats.merge(movies['movieId'])\n",
    "best_movies_sorted = movie_stats.sort_values('bayesian_avg', ascending=False) \n",
    "# On obtient le classement des meilleurs films avec la moyenne bayesian, c'est à dire la moyenne des notations \n",
    "# pondérée du nombre de votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on réduit le DataFrame movies en ne gardant que les k (= nombre_de_films) films les mieux notés\n",
    "\n",
    "movies_reduced = movies.merge(right = best_movies_sorted[:nombre_de_films], on = 'movieId', how = 'right')\n",
    "\n",
    "# on enlève la moyenne normale et le décompte\n",
    "\n",
    "movies_reduced = movies_reduced.drop(columns=['mean', 'count'])\n",
    "\n",
    "# Notre base de donnée de films est prête pour utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on diminue le dataframe ratings avec prise en compte que des k folms de movies_reduced - 2.5s\n",
    "\n",
    "ratings_reduced = movies_reduced.merge(ratings) \n",
    "\n",
    "# on diminue ratings_reduced en ne gardant que les p users ayant effectué le plus de notations\n",
    "liste = list(ratings_reduced.groupby('userId').agg('count').sort_values(by = 'rating', ascending = False)[:nb_users].index) \n",
    "ratings_reduced = ratings_reduced[ratings_reduced['userId'].isin(liste)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On réindexe les matrices movies_reduced et rating_reduced\n",
    "\n",
    "# Création d'un dico pour faire des nouveaux id aux items + création nouvelle colonne au DataFrame\n",
    "liste_movieid = list(movies_reduced.movieId.unique())\n",
    "new_movieid = [i for i in range(len(movies_reduced.movieId.unique()))]\n",
    "dico_movie = dict(zip(liste_movieid, new_movieid))\n",
    "\n",
    "# Création d'un dico pour faire des nouveaux id aux user_id + création nouvelle colonne au DataFrame\n",
    "liste_userid = list(ratings_reduced.userId.unique())\n",
    "new_userid = [i for i in range(len(ratings_reduced.userId.unique()))]\n",
    "dico_user = dict(zip(liste_userid, new_userid))\n",
    "\n",
    "\n",
    "# Il est primordial de comprendre ici qu'il ne faut pas faire de boucle !!! Il faut utiliser les super propriétés de pandas. \n",
    "# Ici la boucle tournerait en énormément de temps, alors que là c'est quasiment instantané.\n",
    "\n",
    "movies_reduced['new_movieid'] = movies_reduced.movieId.apply(lambda x : dico_movie[x])\n",
    "ratings_reduced['new_movieid'] = ratings_reduced.movieId.apply(lambda x : dico_movie[x])\n",
    "ratings_reduced['new_userid'] = ratings_reduced.userId.apply(lambda x : dico_user[x])\n",
    "\n",
    "\n",
    "\n",
    "# On change la position des colonnes du df pour se remettre dans la configuration voulue\n",
    "movies_reduced = movies_reduced.reindex(columns=[\"new_movieid\",  \"title\", \"genres\", \"tconst\", \"bayesian_avg\"])\n",
    "movies_reduced = movies_reduced.rename(columns={'new_movieid' : 'movieId'})\n",
    "ratings_reduced = ratings_reduced.reindex(columns=[\"new_userid\", \"new_movieid\", \"rating\"])\n",
    "ratings_reduced = ratings_reduced.rename(columns={'new_movieid' : 'movieId', 'new_userid' : 'userId'})\n",
    "ratings_reduced = ratings_reduced.reset_index()\n",
    "ratings_reduced = ratings_reduced.drop(columns='index')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbf -> content_based filtering\n",
    "movies_cbf = movies_reduced.drop(columns=['bayesian_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on reste plutôt cablée film américain. On assume ce choix, quitte à un jour faire une version française. Mais on a classé les films par \n",
    "#la moyenne bayesian ce qui nous laisse forcément que des films américains. Donc on regarde les actors dans le top monde.\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "url_actors = 'https://www.imdb.com/list/ls050274118/'\n",
    "url_actresses = 'https://www.imdb.com/list/ls000055475/'\n",
    "url_directors = 'https://www.imdb.com/list/ls053823383/'\n",
    "page_actors = requests.get(url_actors)\n",
    "page_actresses = requests.get(url_actresses)\n",
    "page_directors = requests.get(url_directors)\n",
    "\n",
    "soup_actors = bs(page_actors.content, \"lxml\")\n",
    "soup_actresses = bs(page_actresses.content, \"lxml\")\n",
    "soup_directors = bs(page_directors.content, \"lxml\")\n",
    "\n",
    "actor_ids = soup_actors.find_all('div', class_ ='lister-item-image')\n",
    "actress_ids = soup_actresses.find_all('div', class_ ='lister-item-image')\n",
    "director_ids = soup_directors.find_all('div', class_ ='lister-item-image')\n",
    "\n",
    "list_best_actors = []\n",
    "list_best_actresses = []\n",
    "list_best_directors = []\n",
    "\n",
    "\n",
    "for i in range(len(actor_ids)):\n",
    "    list_best_actors.append(actor_ids[i].find('a')['href'].split('/')[2])\n",
    "\n",
    "for i in range(len(actress_ids)):\n",
    "    list_best_actresses.append(actress_ids[i].find('a')['href'].split('/')[2])\n",
    "\n",
    "for i in range(len(director_ids)):\n",
    "    list_best_directors.append(director_ids[i].find('a')['href'].split('/')[2])\n",
    "\n",
    "# on repère les doublons éventuels\n",
    "\n",
    "for i in list_best_actors:\n",
    "    for j in list_best_directors:\n",
    "        if i == j:\n",
    "            list_best_directors.remove(j)\n",
    "            break\n",
    "\n",
    "for i in list_best_actresses:\n",
    "    for j in list_best_directors:\n",
    "        if i == j:\n",
    "            list_best_directors.remove(j)\n",
    "            break\n",
    "        \n",
    "#TIME -  12s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitement du dataset movies\n",
    "\n",
    "movies_cbf.genres = movies_cbf.genres.apply(lambda x : x.split('|'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on lie movies_reduced avec les principaux personnages de chaque film.\n",
    "movies_cbf = movies_cbf.merge(right = title_principals, on = 'tconst', how = 'left') #15s\n",
    "movies_cbf = movies_cbf.drop(columns=['job', 'characters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_iter = []\n",
    "liste_globale = []\n",
    "\n",
    "for i in range(nombre_de_films) :\n",
    "    for j in movies_cbf[movies_cbf.movieId == i].nconst :\n",
    "        liste_iter.append(j)\n",
    "    liste_globale.append(liste_iter)\n",
    "    liste_iter = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cbf['people'] = movies_cbf.movieId.apply(lambda x : liste_globale[x])\n",
    "movies_cbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cbf = movies_cbf.drop(columns=['nconst', 'ordering', 'category'])\n",
    "movies_cbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cbf = movies_cbf.drop_duplicates(subset=['movieId']).reset_index().drop(columns='index')\n",
    "movies_cbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Des personnages principaux on ne garde que les actor / actress / director\n",
    "\n",
    "#movies_cbf = movies_cbf[(movies_cbf.category == 'director') | (movies_cbf.category == 'actor') | (movies_cbf.category == 'actress')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cbf = movies_cbf.drop(columns=['movieId', 'title', 'tconst'])\n",
    "movies_cbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#movies.genres = movies.genres.apply(lambda x : x.split('|')) - 35s\n",
    "lis_genres = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime','Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', \n",
    "              'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western', '(no genres listed)']\n",
    "\n",
    "lis_totale = list_best_actors + list_best_actresses + list_best_directors\n",
    "\n",
    "\n",
    "for i in lis_genres:\n",
    "    movies_cbf[i] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in movies_cbf.columns[9:]:\n",
    "    for j in movies_cbf.index:\n",
    "        for k in movies_cbf.genres[j]:\n",
    "            movies_cbf.loc[j,k] = 1 if k in lis_genres else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cbf = movies_cbf.drop(columns='genres')\n",
    "movies_cbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movies.genres = movies.genres.apply(lambda x : x.split('|')) - 35s\n",
    "\n",
    "lis_totale = list_best_actors + list_best_actresses + list_best_directors\n",
    "\n",
    "\n",
    "for i in lis_totale:\n",
    "    movies_cbf[i] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25s\n",
    "for i in movies_cbf.columns:\n",
    "    for j in movies_cbf.index:\n",
    "        for k in movies_cbf.people[j]:\n",
    "            if k in lis_totale :\n",
    "                movies_cbf.loc[j,k] = 1  \n",
    "            else :\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_cbf = movies_cbf.drop(columns = 'people')\n",
    "movies_cbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_reduced[movies_reduced['title'].str.contains('Khaled')] # Code à conserver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies_reduced[movies_reduced['title'].str.contains('Godfather')] # Code à conserver\n",
    "\n",
    "# Le code ci-dessous permet de tester le modèle avec différents films\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_sim = cosine_similarity(movies_cbf, movies_cbf)\n",
    "print(f\"Dimensions of our genres cosine similarity matrix: {cosine_sim.shape}\")\n",
    "\n",
    "# Code permettant de trouver le nom d'un film en fonction de son \n",
    "\n",
    "def movie_finder(title):\n",
    "    return movies_reduced[movies_reduced['title'].str.contains(title)]['title'].tolist()\n",
    "\n",
    "movie_idx = dict(zip(movies_reduced['title'], list(movies_reduced.index))) # dictionnaire clé = titre, valeur = index du titre\n",
    "title = movie_finder('Django')[0]\n",
    "n_recommendations = 10\n",
    "\n",
    "idx = movie_idx[title] # index du titre du film\n",
    "sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "sim_scores = sim_scores[1:(n_recommendations+1)]\n",
    "similar_movies = [i[0] for i in sim_scores]\n",
    "\n",
    "print(f\"Recommendations for {title}:\")\n",
    "movies_reduced['title'].iloc[similar_movies]\n",
    "\n",
    "# ATTENTION A LA RÉINDEXATION. IL FAUT S'ASSURER QUE CELA NE PERTURBE PAS LA SUITE DES OPÉRATIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas n°1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Action = 1\n",
    "Adventure = 1\n",
    "Animation = 0\n",
    "Children = 0\n",
    "Comedy = 0\n",
    "Crime = 0\n",
    "Documentary = 0\n",
    "Drama = 0\n",
    "Fantasy = 0\n",
    "Film_Noir = 0\n",
    "Horror = 0\n",
    "IMAX = 1\n",
    "Musical = 0\n",
    "Mystery = 0\n",
    "Romance = 0\n",
    "Sci_Fi = 0\n",
    "Thriller = 1\n",
    "War = 0\n",
    "Western = 0\n",
    "no_genres_listed = 0\n",
    "\n",
    "\n",
    "new_line = pd.DataFrame([[Action, Adventure, Animation, Children, Comedy, Crime,Documentary, Drama, Fantasy, Film_Noir, Horror, IMAX, \n",
    "              Musical, Mystery, Romance, Sci_Fi, Thriller, War, Western, no_genres_listed]], columns=['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime','Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', \n",
    "              'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western', '(no genres listed)'])\n",
    "\n",
    "# Fusion des deux DataFrame\n",
    "movies_cbf_user = pd.concat([movies_cbf.iloc[:,:20], new_line], ignore_index=True)\n",
    "movies_cbf_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity # La matrice cosinus permet d'évaluer le degré de similarité entre 2 vecteurs.\n",
    "\n",
    "cosine_sim = cosine_similarity(movies_cbf_user, movies_cbf_user)\n",
    "print(f\"Dimensions of our genres cosine similarity matrix: {cosine_sim.shape}\")\n",
    "\n",
    "n_recommendations = 10\n",
    "\n",
    "title = 'Visiteur'\n",
    "idx = nombre_de_films # index du titre du film. A REMPLACER PAR UNE VARIABLE\n",
    "\n",
    "sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "sim_scores = list(filter(lambda x: x[0] != nombre_de_films, sim_scores)) # on enlève de la liste le tuple correspondant au film de base\n",
    "sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "similar_movies = [i[0] for i in sim_scores]\n",
    "\n",
    "\n",
    "print(f\"Recommendations for {title}:\")\n",
    "movies_reduced['title'].iloc[similar_movies]\n",
    "\n",
    "# ATTENTION A LA RÉINDEXATION. IL FAUT S'ASSURER QUE CELA NE PERTURBE PAS LA SUITE DES OPÉRATIONS\n",
    "# Les films qui sortent sont classés, par construction, suivant leur moyenne bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas n°2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix # L'idée ici est de générer une matrice de notations avec les users en ligne et les films en colonne\n",
    "\n",
    "def create_X(df):\n",
    "    \"\"\"\n",
    "    Generates a sparse matrix from ratings dataframe.\n",
    "\n",
    "    Args:\n",
    "        df: pandas dataframe containing 3 columns (userId, movieId, rating)\n",
    "\n",
    "    Returns:\n",
    "        X: sparse matrix\n",
    "        user_mapper: dict that maps user id's to user indices\n",
    "        user_inv_mapper: dict that maps user indices to user id's\n",
    "        movie_mapper: dict that maps movie id's to movie indices\n",
    "        movie_inv_mapper: dict that maps movie indices to movie id's\n",
    "    \"\"\"\n",
    "    M = df['userId'].nunique()  # nombre de users\n",
    "    N = df['movieId'].nunique() # nombre de films - On définit ainsi la dimension de la matrice\n",
    "\n",
    "    user_mapper = dict(zip(np.unique(df[\"userId\"]), list(range(M))))\n",
    "    movie_mapper = dict(zip(np.unique(df[\"movieId\"]), list(range(N))))\n",
    "\n",
    "    user_inv_mapper = dict(zip(list(range(M)), np.unique(df[\"userId\"])))\n",
    "    movie_inv_mapper = dict(zip(list(range(N)), np.unique(df[\"movieId\"])))\n",
    "\n",
    "    user_index = [user_mapper[i] for i in df['userId']]\n",
    "    item_index = [movie_mapper[i] for i in df['movieId']]\n",
    "\n",
    "    X = csr_matrix((df[\"rating\"], (user_index,item_index)), shape=(M,N))\n",
    "\n",
    "    return X, user_mapper, movie_mapper, user_inv_mapper, movie_inv_mapper\n",
    "\n",
    "X, user_mapper, movie_mapper, user_inv_mapper, movie_inv_mapper = create_X(ratings_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul of Matrix sparcity. La sparse matrix est une matrice où la plupart des éléments sont des zéros\n",
    "\n",
    "n_total = X.shape[0]*X.shape[1] #Nombre total d'inputs de la matrice\n",
    "n_ratings = X.nnz               # on repère les inputs non nuls de la matrice\n",
    "sparsity = n_ratings/n_total    # On calcul le taux de remplissage de la matrice\n",
    "print(f\"Matrix sparsity: {round(sparsity*100,2)}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ratings_per_user = X.getnnz(axis=1) # On récupère le nombre de votes en ligne, c'est à dire par user\n",
    "\n",
    "print(f\"Most active user rated {n_ratings_per_user.max()} movies.\")\n",
    "print(f\"Least active user rated {n_ratings_per_user.min()} movies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ratings_per_movie = X.getnnz(axis=0) # On récupère le nombre de votes en colonne, c'est à dire par film\n",
    "\n",
    "print(f\"Most rated movie has {n_ratings_per_movie.max()} ratings.\")\n",
    "print(f\"Least rated movie has {n_ratings_per_movie.min()} ratings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisation des données. Les données nulles sont remplacées par l'opposé des moyennes. POURQUOI ?\n",
    "\n",
    "sum_ratings_per_movie = X.sum(axis=0)\n",
    "mean_rating_per_movie = sum_ratings_per_movie/n_ratings_per_movie\n",
    "X_mean_movie = np.tile(mean_rating_per_movie, (X.shape[0],1))\n",
    "X_norm = X - csr_matrix(X_mean_movie) # 14s\n",
    "\n",
    "print(\"Original X:\", X[0].todense())\n",
    "print(\"Normalized X:\", X_norm[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTRUCTION DE L'ALGORITHME KNN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def find_similar_movies(movie_id, X, movie_mapper, movie_inv_mapper, k, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Finds k-nearest neighbours for a given movie id.\n",
    "\n",
    "    Args:\n",
    "        movie_id: id of the movie of interest\n",
    "        X: user-item utility matrix\n",
    "        k: number of similar movies to retrieve\n",
    "        metric: distance metric for kNN calculations\n",
    "\n",
    "    Output: returns list of k similar movie ID's\n",
    "    \"\"\"\n",
    "    X = X.T\n",
    "    neighbour_ids = []\n",
    "\n",
    "    movie_ind = movie_mapper[movie_id]  # on cherche l'indice de ce film dans la matrice\n",
    "    movie_vec = X[movie_ind]            # on trouve ainsi le vecteur de notations correspondant à ce film\n",
    "    if isinstance(movie_vec, (np.ndarray)): # on cherche à savoir si movie vec est un array ???\n",
    "        movie_vec = movie_vec.reshape(1,-1)\n",
    "    # use k+1 since kNN output includes the movieId of interest\n",
    "    kNN = NearestNeighbors(n_neighbors=k+1, algorithm=\"brute\", metric=metric)\n",
    "    kNN.fit(X)\n",
    "    neighbour = kNN.kneighbors(movie_vec, return_distance=False)\n",
    "    for i in range(0,k):\n",
    "        n = neighbour.item(i)\n",
    "        neighbour_ids.append(movie_inv_mapper[n])\n",
    "    neighbour_ids.pop(0)\n",
    "    return neighbour_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SORTIES DE L'ALGORITHME KNN -- 1 min 30\n",
    "# sur quelle base est-ce que knn donne des résultats : Il va chercher dans la matrice X, où on a remplacé les nulle par des - moyenne. \n",
    "#Il va chercher les ratings les plus proches, tout simplement. On peut dire que son approche prend en compte le nombre de votes puisque \n",
    "# tous les votes nuls sont pénalisants.\n",
    "# Néanmmoins, l'approche kNN sur du filtrage collaboratif simple revient à classer avec une moyenne bayesian.\n",
    "# Il n'est pas possible d'évaluer cette approche,  puisqu'on ne cherche pas ici à prédire une note, on cherche simplement les k voisins. \n",
    "\n",
    "\n",
    "movie_titles = dict(zip(movies_reduced['movieId'], movies_reduced['title'])) # On associe les movieId avec leur titre dans un dictionnaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_reduced[movies_reduced['title'].str.contains('Godfather')] # Code à conserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "movie_id = 1 \n",
    "\n",
    "similar_movies = find_similar_movies(movie_id, X_norm, movie_mapper, movie_inv_mapper, metric='cosine', k=10)\n",
    "\n",
    "movie_title = movie_titles[movie_id]\n",
    "\n",
    "print(f\"Because you watched {movie_title}:\")\n",
    "for i in similar_movies:\n",
    "    print(movie_titles[i]) # problème d'index\n",
    "\n",
    "# Faire une critique du résultat obtenu par kNN en le comparant à un classement bayesian, et ensuite passer au content-based Filtering qui\n",
    "# devrait nous permettre de faire des évaluations.\n",
    "\n",
    "# Tourne en  \n",
    "\n",
    "# AJOUTER LES DISTANCES \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas n°3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On se base sur un profil d'utilisateur qui va entrer des notes et rentrer dans la base de notation\n",
    "# On propose l'utilisateur \n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test, split\n",
    "train, test = train_test_split(ratings_reduced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Nombre d'éléments uniques\n",
    "num_users = ratings_reduced['userId'].nunique()\n",
    "num_movies = ratings_reduced['movieId'].nunique()\n",
    "\n",
    "# Modèle\n",
    "user_input = Input(shape=(1,))\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=10)(user_input)\n",
    "user_flatten = Flatten()(user_embedding)\n",
    "\n",
    "item_input = Input(shape=(1,))\n",
    "item_embedding = Embedding(input_dim= num_movies, output_dim=10)(item_input)\n",
    "item_flatten = Flatten()(item_embedding)\n",
    "\n",
    "dot_product = Dot(axes=1)([user_flatten, item_flatten])\n",
    "\n",
    "# Add a dense layer for final prediction\n",
    "hidden_layer = Dense(64, activation='relu')(dot_product)\n",
    "output_layer = Dense(1)(hidden_layer)\n",
    "\n",
    "model = Model(inputs=[user_input, item_input], outputs=output_layer)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics = [\"mae\"])\n",
    "\n",
    "model.fit([train['userId'], train['movieId']], train['rating'], epochs=3, batch_size=32, validation_data=([test['userId'], test['movieId']], test['rating']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions\n",
    "predictions = model.predict([test['userId'], test['movieId']])\n",
    "pd.DataFrame(predictions).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from surprise import Dataset \n",
    "from surprise import Reader\n",
    "from surprise.model_selection import train_test_split \n",
    "from surprise.model_selection import cross_validate \n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import NormalPredictor \n",
    "from surprise import BaselineOnly \n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans \n",
    "from surprise import KNNWithZScore \n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD \n",
    "from surprise import SVDpp \n",
    "from surprise import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "#df = pd.read_csv('u.data', sep='\\t', names=column_names)\n",
    "\n",
    "#movie_titles = pd.read_csv(\"Movie_Id_Titles\")\n",
    "\n",
    "#df = pd.merge(df,movie_titles,on='item_id')\n",
    "\n",
    "n_users = ratings_reduced.userId.nunique()\n",
    "n_movies = ratings_reduced.movieId.nunique()\n",
    "\n",
    "print('Num. of Users: '+ str(n_users))\n",
    "print('Num of Movies: '+str(n_movies))\n",
    "\n",
    "#df = df.rename(columns={'user_id' : 'userId', 'item_id' : 'movieId'})\n",
    "\n",
    "\n",
    "# Call Reader and set Rating Scale from 0.5 to 5\n",
    "reader = Reader(rating_scale= (0.5, 5))\n",
    "# Parse data and select only necessary files\n",
    "data = Dataset.load_from_df(ratings_reduced[['userId', 'movieId','rating']], reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into 75% / 25%\n",
    "trainset, testset = train_test_split(data, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run svd algroichm\n",
    "model = SVD()\n",
    "# This follows the typical sklearn train and test model building\n",
    "model.fit(trainset)\n",
    "predictions = model.test(testset)\n",
    "# compute erros accuracy.rmse(predictions)\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mse(predictions)\n",
    "accuracy.mae(predictions)\n",
    "accuracy.fcp(predictions)\n",
    "\n",
    "# 34 min sur data frame complet, RMSE 0.7889"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
